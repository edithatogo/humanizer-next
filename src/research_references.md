
## RESEARCH AND EXTERNAL SOURCES

While Wikipedia's "Signs of AI writing" remains a primary community-maintained source, the following academic and technical resources provide additional patterns and grounding for detection and humanization:

### 1. Academic Studies on Detection Unreliability
- **University of Illinois / University of Chicago:** Research highlighting that AI detectors disproportionately flag non-native English speakers due to "textual simplicity" and overpromise accuracy while failing to detect paraphrased content.
- **University of Maryland:** Studies on the "Watermarking" vs. "Statistical" detection methods, emphasizing that as LLMs evolve, statistical signs (like those documented here) become harder to rely on without human judgment.

### 2. Technical Metrics: Perplexity and Burstiness (GPTZero)
- **Perplexity:** A measure of randomness. AI tends toward low perplexity (statistically predictable word choices). Humanizing involves using more varied, slightly less "optimized" vocabulary.
- **Burstiness:** A measure of sentence length variation. Humans write with inconsistent rhythmsâ€”short punchy sentences followed by long complex ones. AI tends toward a uniform, "un-bursty" rhythm.

### 3. Linguistic Hallmarks (Originality.ai)
- **Tautology and Redundancy:** AI often restates the same point using slightly different synonyms to fill space or achieve a target length.
- **Unicode Artifacts:** Some detectors look for specific non-printing characters or unusual font-encoding artifacts that LLMs sometimes produce.

### 4. Overused "Tells" (Collective Community Observations)
- High-frequency occurrences of: "delve", "tapestry", "landscape", "at its core", "not only... but also", "in summary", "moreover", "furthermore".
