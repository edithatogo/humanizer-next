# AI Benchmarks (SQuAD, GLUE, SuperGLUE, CoNLL)

**Sources:** gluebenchmark.com, rajpurkar.github.io/SQuAD-explorer

**Accessed:** 2026-01-31

## Summary

These datasets establish the "Human Baseline" against which AI models are measured.

## Key Datasets

1. **SQuAD:** Reading comprehension (Q&A).
2. **GLUE/SuperGLUE:** General Language Understanding Evaluation (Entailment, Sentiment, Similarity).
3. **CoNLL-2003:** Named Entity Recognition.

## Relevance

AI models achieving "superhuman" performance on these benchmarks (e.g., F1 > 90%) often exhibit the "Perfect Grammar" and "Formulaic Structure" signs identified in detection.
